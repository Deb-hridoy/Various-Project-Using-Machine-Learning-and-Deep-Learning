# -*- coding: utf-8 -*-
"""Algorithm Based Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_fr4H4miBttfqCt8huP0X0DIWEeMoT8W

## Decission Tree
"""

from google.colab import files
upload_data = files.upload()

import pandas as pd
import numpy as np
import sklearn
from sklearn import tree
import matplotlib.pyplot as plt

df = pd.read_csv('male female.csv')

df.shape

df.head()

x_data= df.iloc[:,:-1]    # iloc[row, coloumn]

x_data.head()

y_data = df.iloc[:, 3]
y_data.head()

from sklearn.tree import DecisionTreeClassifier

model_0 = tree.DecisionTreeClassifier()
model_0

model_fit = model_0.fit(x_data, y_data)

model_predict = model_fit.predict([[177,	70,	43]])

model_predict

from sklearn import tree
tree.plot_tree(model_0, filled=True, rounded=True, feature_names=x_data.columns)

"""## Decision Tree Classification"""

from google.colab import files
upload_data = files.upload()

import pandas as pd

df_1 = pd.read_csv('shop data.csv')

df_1.head()

x_data = df_1.iloc[:, :-1]
x_data.head()

y_data = df_1.iloc[:, 4]
y_data.head()

"""### Data preprcessing for string data"""

from sklearn.preprocessing import LabelEncoder

label_x = LabelEncoder()

x_data_label = x_data.apply(label_x.fit_transform)
#x_data_label = x_data.apply(LabelEncoder().fit_transform)
x_data_label.head()

"""### Prediction"""

from sklearn.tree import DecisionTreeClassifier
import numpy as np

model_1 = DecisionTreeClassifier()
model_1

model_1_fit = model_1.fit(x_data_label, y_data)

x_input = np.array([1, 0, 0, 0])   # <25, high, male, single

model_1_predict = model_1_fit.predict([x_input])

model_1_predict

plt.figure(figsize=(10, 10))
tree.plot_tree(model_1, filled=True, rounded=True, feature_names=x_data.columns)
plt.show()
plt.savefig('Tree.png')

"""##  Make a classification model to know about the Confusion Matrix"""

import seaborn as sns
from sklearn.datasets import make_classification   # for regression sklearn module is make_regression
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report

X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)

X_df = pd.DataFrame(X, columns=[f'Feature {i}' for i in range(X.shape[1])])
X_df.head()

X.shape, y.shape

x_train, x_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=42)

feature_index = 0
plt.scatter(X[:,feature_index],y)
plt.xlabel(f'Feature {feature_index}')
plt.ylabel('Target')
plt.show()

model_2 = tree.DecisionTreeClassifier()
model_2_fit = model_2.fit(x_train, y_train)
y_predict = model_2_fit.predict(x_test)

"""### Confusion matrix"""

confu_matrix = confusion_matrix(y_test, y_predict)
confu_matrix

plt.figure(figsize=(10,6))
sns.heatmap(confu_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative','Predicted Positive'], yticklabels=['Actual Negative', 'Actual Postive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""### Split the confusion matrix into True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN)

The ravel() method is used to flatten the confusion matrix into a 1D array
"""

tn, fp, fn, tp = confu_matrix.ravel()

print(f"True Positive:{tp}")
print(f"False Positive:{fp}")
print(f"False Negative:{fn}")
print(f"True Negative:{tn}")

"""### Various performance matrics"""

accuracy = accuracy_score(y_test, y_predict)
precision = precision_score(y_test, y_predict)
recall = recall_score(y_test, y_predict)
f1 = f1_score(y_test, y_predict)
classifi_report = classification_report(y_test, y_predict)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 score: {f1:.4f}")

print(f"Classification Report:{classifi_report}")

"""### Display ROC curve"""

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_predictions(y_test, y_predict)
plt.plot([0,1], [0,1])
plt.show()

"""## Model create for K fold cross validation"""

import xgboost as xgb
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score
import warnings
warnings.filterwarnings('ignore')

X, y =make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42, scale=6)

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

plt.figure(figsize=(4,4))
plt.scatter(X[:,0], y, c='r')
plt.scatter(X[:,1], y, c='b')
plt.scatter(X[:,2], y, c='g')
plt.show()

model_3 = xgb.XGBClassifier(use_label_encoder=False, eval_metrics='logloss')
model_3

model_3_fit = model_3.fit(x_train, y_train)

model_3_fit.score(x_test, y_test)

"""### Perform K-Fold Cross-Validation"""

k_fold_cross = KFold(n_splits=5, random_state=42, shuffle=True)
kf_scores = cross_val_score(model_3, X, y, cv=k_fold_cross)

kf_scores

kf_scores.mean()

"""### Stratified K-fold Cross-validation"""

stratified_cross_validation = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
stratified_scores = cross_val_score(model_3, X, y, cv=stratified_cross_validation)

stratified_scores

stratified_scores.mean()

"""## Logistic Regression model"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files
upload_data = files.upload()

df_4 = pd.read_csv('marital status.csv')

df_4.head()

df_4.isnull().sum()

handle = df_4['status'].median()

handle

"""### Fill the missing value"""

df_4.status = df_4.status.fillna(handle)

df_4.isnull().sum()

df_4['status'].value_counts()

x_data_4 = df_4[['age']]

x_data_4.head()

y_data_4 = df_4[['status']]

y_data_4.head()

x_train, x_test, y_train, y_test = train_test_split(x_data_4, y_data_4, test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression

model_4 = LogisticRegression()

model_4

model_4_fit = model_4.fit(x_train, y_train)

y_predict = model_4_fit.predict(x_test)

y_predict

y_test

Accuracy = model_4.score(x_test, y_test)

Accuracy

"""### model predictions probability"""

model_4_fit.predict_proba(x_test)

"""## K-Nearest Neighbors"""

# first feature
weather_conditions = [
    "Clear", "Partly Cloudy", "Cloudy", "Rain", "Sunny",
    "Rain", "Thunderstorm", "Snow", "Sunny", "Fog",
    "Mist", "Drizzle", "Windy", "Fog", "Haze"
]

# Second feature
temperature_conditions = [
    "Freezing", "Cold", "Cool", "Mild", "Warm",
    "Hot", "Scorching", "Chilly", "Cold", "Warm",
    "Mild", "Hot", "Cool", "Scorching", "Chilly"
]

# label or target variable
play_list = [
    "Yes", "No", "Yes", "No", "Yes",
    "No", "Yes", "No", "Yes", "No",
    "Yes", "No", "Yes", "No", "Yes"
]

from sklearn.preprocessing import LabelEncoder

encoded = LabelEncoder()

encod_weather = encoded.fit_transform(weather_conditions)

encod_weather

encod_temp = encoded.fit_transform(temperature_conditions)

encod_temp

encod_label = encoded.fit_transform(play_list)

encod_label

features =list(zip(encod_weather, encod_temp))

from sklearn.neighbors import KNeighborsClassifier

model_5 = KNeighborsClassifier()

model_5_fit = model_5.fit(features, encod_label)

model_predicted = model_5_fit.predict([[0, 3]])

print(model_predicted)

"""## Count Vectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

dataset = ["Hey welcome to my channel Hridoy Deb Mahin",
           "Hey i am Hridoy",
           "I love the language python",
           "Anthing can solve with the programming language",
           "I love to do the progrmming in python language"]

cv = CountVectorizer()
cv

convert_data = cv.fit_transform(dataset)

cv.get_feature_names_out()

convert_data.toarray()

"""## TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

tfidf_fit = tfidf.fit_transform(dataset)

tfidf_fit.toarray()

